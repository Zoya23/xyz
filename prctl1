# Code Generated by Sidekick is for learning and experimentation purposes only.
pip install nltk

# text_preprocessing packages
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Download stopwords
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

# Function for Tokenization
def tokenize(text):
    """Tokenizes the input text into words."""
    return word_tokenize(text)

# Function for Stop-words Removal
def remove_stopwords(tokens):
    """Removes stop words from the list of tokens."""
    stop_words = set(stopwords.words('english'))
    return [word for word in tokens if word.lower() not in stop_words]

# Function for Text Normalization
def normalize(text):
    """Normalizes the text by converting it to lowercase and removing punctuation."""
    text = text.lower()  # Convert to lowercase
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    return text

# Pre-processing Pipeline Function
def preprocess_text(text):
    """Combines all text preprocessing steps."""
    normalized_text = normalize(text)  # this is goa it has beaches
    tokens = tokenize(normalized_text)  # this, is, goa, it, has, beaches
    # print(tokens)
    filtered_tokens = remove_stopwords(tokens)  # goa beaches
    return ' '.join(filtered_tokens)  # Return as a single string

# preprocess_text("This is Goa . It has beaches")
input_file_path = '/content/SMSSpamCollection'
output_file_path = 'processed_output.txt'

with open(input_file_path, 'r', encoding='utf-8') as file:
    lines = file.readlines()

# Process each line and store results
processed_lines = [preprocess_text(line.strip()) for line in lines]

# Save the processed lines to a new text file
with open(output_file_path, 'w', encoding='utf-8') as file:
    for line in processed_lines:
        file.write(line + '\n')

print(f"Text preprocessing completed and saved to '{output_file_path}'.")

# Print the first 5 lines of the input file
print("Provided Input (First 5 Lines):")
with open(input_file_path, 'r', encoding='utf-8') as file:
    for _ in range(5):
        line = file.readline()
        if line:  # Check if there's a line to read
            print(line.strip())
        else:
            break  # Exit if there are fewer than 5 lines
