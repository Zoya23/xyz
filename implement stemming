# Code Generated by Sidekick is for learning and experimentation purposes only.
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import string

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load the dataset
data = {'messages': [
    "The cats are playing in the garden.",
    "He is running quickly to catch the bus.",
    "The boys are enjoying their game.",
    "She was reading a book.",
    "I love to eat apples and bananas."
]}
df = pd.DataFrame(data)

# Initialize the stemmer and stop words
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)
    # Stop-words Removal
    tokens = [word for word in tokens if word.lower() not in stop_words]
    # Text Normalization (Lowercasing and Removing punctuation)
    tokens = [word.lower() for word in tokens if word.isalnum()]
    # Stemming
    tokens = [stemmer.stem(word) for word in tokens]
    return ' '.join(tokens)

# Apply the pre-processing pipeline
df['processed_messages'] = df['messages'].apply(preprocess_text)

# Display the result
print(df[['messages', 'processed_messages']])
print("\n \n")

